import streamlit as st
import pandas as pd
import os
import json

# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
SAVE_DIR = "uploaded_files"
STATE_FILE = os.path.join(SAVE_DIR, "state.json")
os.makedirs(SAVE_DIR, exist_ok=True)

# ---------------------------- çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç† ----------------------------
def load_state():
    """çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å­˜åœ¨ã—ãªã„å ´åˆã¯åˆæœŸçŠ¶æ…‹ã‚’è¿”ã—ã¾ã™ã€‚"""
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"prev_file": None, "curr_file": None, "helper_file": None}

def save_state(state):
    """çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã™ã€‚"""
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

# ---------------------------- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ----------------------------

def read_uploaded_file(uploaded_file):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚·ãƒ¼ãƒˆåã‚’ã‚­ãƒ¼ã€DataFrameã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ã‚’è¿”ã—ã¾ã™ã€‚
    ãƒ•ã‚¡ã‚¤ãƒ«ãŒNoneã®å ´åˆã¯ç©ºã®è¾æ›¸ã‚’è¿”ã—ã¾ã™ã€‚
    """
    if uploaded_file is not None:
        return pd.read_excel(uploaded_file, sheet_name=None, header=None)
    return {}

def save_file_and_update_state(uploaded_file, file_key):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ã€çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¾ã™ã€‚
    """
    if uploaded_file:
        filepath = os.path.join(SAVE_DIR, uploaded_file.name)
        with open(filepath, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.session_state.state[file_key] = {"path": filepath, "name": uploaded_file.name}
        save_state(st.session_state.state)

def extract_mapping(helper_sheets):
    """
    è£œåŠ©ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆã‹ã‚‰ã€é™¤å¤–ã‚³ãƒ¼ãƒ‰ã€å£²ä¸Šä¿®æ­£ãƒãƒƒãƒ—ã€ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ—ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    """
    exclude_codes = []
    if "å‰Šé™¤ä¾é ¼" in helper_sheets:
        codes = helper_sheets["å‰Šé™¤ä¾é ¼"].iloc[:, 0].dropna()
        codes = codes.astype(str).str.replace(r"\.0$", "", regex=True).str.zfill(4)
        exclude_codes = codes.tolist()

    fix_sales_map = {}
    if "è¨ˆç®—ä¿®æ­£" in helper_sheets:
        sheet = helper_sheets["è¨ˆç®—ä¿®æ­£"].iloc[:, :2].dropna(how="all")
        for _, row in sheet.iterrows():
            try:
                code = str(int(row[0])).zfill(4)
                factor = float(row[1])
                fix_sales_map[code] = factor
            except ValueError:
                st.warning(f"ã€Œè¨ˆç®—ä¿®æ­£ã€ã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒä¸æ­£ã§ã™: {row.tolist()}")
                continue

    category_map = {}
    if "å–å¼•å…ˆãƒªã‚¹ãƒˆ" in helper_sheets:
        sheet = helper_sheets["å–å¼•å…ˆãƒªã‚¹ãƒˆ"].iloc[:, [0, 2]].dropna(how="all")
        for _, row in sheet.iterrows():
            try:
                code = str(int(row[0])).zfill(4)
                category = str(row[2]).strip()
                category_map[code] = category
            except ValueError:
                st.warning(f"ã€Œå–å¼•å…ˆãƒªã‚¹ãƒˆã€ã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒä¸æ­£ã§ã™: {row.tolist()}")
                continue

    return exclude_codes, fix_sales_map, category_map

def clean_sheet(df, exclude_codes, fix_sales_map, category_map):
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€å¿…è¦ãªåˆ—ã‚’æ•´å½¢ã—ã¾ã™ã€‚
    """
    header_idx = df[df.apply(lambda r: r.astype(str).str.contains("å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", na=False)).any(axis=1)].index
    if len(header_idx) == 0:
        return pd.DataFrame()
    header = header_idx[0]

    df.columns = df.iloc[header]
    df = df[(header + 1):].reset_index(drop=True)

    required_columns = {"å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå", "ç´”å£²ä¸Šé¡"}
    if not required_columns.issubset(df.columns):
        return pd.DataFrame()

    df["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"] = df["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"].astype(str).str.replace(r"\.0$", "", regex=True).str.zfill(4)
    df = df[~df["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"].isin(exclude_codes)]

    df["ç´”å£²ä¸Šé¡"] = df.apply(
        lambda r: r["ç´”å£²ä¸Šé¡"] * fix_sales_map.get(r["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"], 1.0),
        axis=1
    )
    df["å¤§åˆ†é¡"] = df["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰"].map(category_map).fillna("æœªåˆ†é¡")

    total_sales = df["ç´”å£²ä¸Šé¡"].sum()
    df["æ§‹æˆæ¯”"] = (df["ç´”å£²ä¸Šé¡"] / total_sales * 100).round(2) if total_sales != 0 else 0.0

    grouped = (
        df.groupby(["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå", "å¤§åˆ†é¡"], as_index=False)
        .agg({"ç´”å£²ä¸Šé¡": "sum", "æ§‹æˆæ¯”": "sum"})
        .sort_values("ç´”å£²ä¸Šé¡", ascending=False)
    )

    return grouped

def compare_years(prev_df, curr_df):
    """
    å‰å¹´ãƒ‡ãƒ¼ã‚¿ã¨ä»Šå¹´ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ã€å·®é¡ã¨å‰å¹´æ¯”ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
    """
    merged = pd.merge(
        prev_df,
        curr_df,
        on=["å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå", "å¤§åˆ†é¡"],
        how="outer",
        suffixes=("_å‰å¹´", "_ä»Šå¹´"),
    )

    for col in ["ç´”å£²ä¸Šé¡_å‰å¹´", "ç´”å£²ä¸Šé¡_ä»Šå¹´", "æ§‹æˆæ¯”_å‰å¹´", "æ§‹æˆæ¯”_ä»Šå¹´"]:
        merged[col] = merged[col].fillna(0)

    merged["ç´”å£²ä¸Šé¡_å‰å¹´"] = (merged["ç´”å£²ä¸Šé¡_å‰å¹´"] / 1000).round().astype("Int64")
    merged["ç´”å£²ä¸Šé¡_ä»Šå¹´"] = (merged["ç´”å£²ä¸Šé¡_ä»Šå¹´"] / 1000).round().astype("Int64")

    merged["å·®é¡"] = merged["ç´”å£²ä¸Šé¡_ä»Šå¹´"] - merged["ç´”å£²ä¸Šé¡_å‰å¹´"]
    merged["å‰å¹´æ¯”(%)"] = merged.apply(
        lambda row: round(row["ç´”å£²ä¸Šé¡_ä»Šå¹´"] / row["ç´”å£²ä¸Šé¡_å‰å¹´"] * 100, 1)
        if row["ç´”å£²ä¸Šé¡_å‰å¹´"] != 0 else (100.0 if row["ç´”å£²ä¸Šé¡_ä»Šå¹´"] != 0 else 0.0),
        axis=1
    )

    ordered_cols = [
        "å¾—æ„å…ˆã‚³ãƒ¼ãƒ‰", "å¾—æ„å…ˆå", "å¤§åˆ†é¡",
        "ç´”å£²ä¸Šé¡_ä»Šå¹´", "æ§‹æˆæ¯”_ä»Šå¹´",
        "ç´”å£²ä¸Šé¡_å‰å¹´", "æ§‹æˆæ¯”_å‰å¹´",
        "å‰å¹´æ¯”(%)", "å·®é¡"
    ]
    return merged[ordered_cols]

def summarize_by_category(comp_df):
    """
    ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆã—ã¾ã™ã€‚
    """
    cat = comp_df.groupby("å¤§åˆ†é¡", as_index=False).agg({
        "ç´”å£²ä¸Šé¡_å‰å¹´": "sum",
        "ç´”å£²ä¸Šé¡_ä»Šå¹´": "sum",
        "å·®é¡": "sum"
    })
    cat["å‰å¹´æ¯”(%)"] = cat.apply(
        lambda r: round(r["ç´”å£²ä¸Šé¡_ä»Šå¹´"] / r["ç´”å£²ä¸Šé¡_å‰å¹´"] * 100, 1)
        if r["ç´”å£²ä¸Šé¡_å‰å¹´"] != 0 else (100.0 if r["ç´”å£²ä¸Šé¡_ä»Šå¹´"] != 0 else 0.0),
        axis=1
    )
    return cat

# ---------------------------- Streamlit ã‚¢ãƒ—ãƒª ----------------------------

st.set_page_config(page_title="å¸å–¶æ¥­æ•°å€¤åˆ†æã‚·ã‚¹ãƒ†ãƒ ", layout="wide")
st.title("ğŸ“Š å¸å–¶æ¥­æ•°å€¤åˆ†æã‚·ã‚¹ãƒ†ãƒ ")

# çŠ¶æ…‹ã®ãƒ­ãƒ¼ãƒ‰
if "state" not in st.session_state:
    st.session_state.state = load_state()

# å·¦å´ã«ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³ã€å³å´ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰UIã‚’é…ç½®
left_col, right_col = st.columns([1, 2])

with left_col:
    st.subheader("ğŸ“‚ ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³")
    file_status_html = "<div style='background-color:#f9f5e9; padding:15px; border-radius:8px; border:1px solid #ddd;'>"
    
    status_map = {
        "prev_file": "å‰å¹´ãƒ‡ãƒ¼ã‚¿",
        "curr_file": "ä»Šå¹´ãƒ‡ãƒ¼ã‚¿",
        "helper_file": "è£œåŠ©ãƒ‡ãƒ¼ã‚¿"
    }
    
    for key, label in status_map.items():
        info = st.session_state.state.get(key)
        if info and "name" in info:
            file_status_html += f"<p><strong>{label}</strong>: âœ… {info['name']}</p>"
        else:
            file_status_html += f"<p><strong>{label}</strong>: âŒ æœªè¨­å®š</p>"
    file_status_html += "</div>"
    st.markdown(file_status_html, unsafe_allow_html=True)
    
with right_col:
    st.header("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    prev_file_uploader = st.file_uploader("å‰å¹´ãƒ‡ãƒ¼ã‚¿ (Excel)", type=["xlsx"], key="prev_file_uploader")
    curr_file_uploader = st.file_uploader("ä»Šå¹´ãƒ‡ãƒ¼ã‚¿ (Excel)", type=["xlsx"], key="curr_file_uploader")
    helper_file_uploader = st.file_uploader("è£œåŠ©ãƒ‡ãƒ¼ã‚¿ (ãƒ‡ãƒ¼ã‚¿æ•´ç†.xlsx)", type=["xlsx"], key="helper_file_uploader")

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    save_file_and_update_state(prev_file_uploader, "prev_file")
    save_file_and_update_state(curr_file_uploader, "curr_file")
    save_file_and_update_state(helper_file_uploader, "helper_file")

# ğŸš€ åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸéš›ã®å‡¦ç†
if st.button("ğŸš€ åˆ†æå®Ÿè¡Œ"):
    # çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ã‚¹ã‚’å–å¾—
    prev_file_path = st.session_state.state.get("prev_file", {}).get("path")
    curr_file_path = st.session_state.state.get("curr_file", {}).get("path")
    helper_file_path = st.session_state.state.get("helper_file", {}).get("path")
    
    if prev_file_path and curr_file_path and helper_file_path:
        try:
            prev_sheets = pd.read_excel(prev_file_path, sheet_name=None, header=None)
            curr_sheets = pd.read_excel(curr_file_path, sheet_name=None, header=None)
            helper_sheets = pd.read_excel(helper_file_path, sheet_name=None, header=None)
            
            exclude_codes, fix_sales_map, category_map = extract_mapping(helper_sheets)

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            if not prev_sheets:
                st.error("å‰å¹´ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                st.stop()
            prev_sheet_df = list(prev_sheets.values())[0]

            if not curr_sheets:
                st.error("ä»Šå¹´ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                st.stop()
            curr_sheet_df = list(curr_sheets.values())[0]

            prev_clean = clean_sheet(prev_sheet_df, exclude_codes, fix_sales_map, category_map)
            curr_clean = clean_sheet(curr_sheet_df, exclude_codes, fix_sales_map, category_map)

            if prev_clean.empty or curr_clean.empty:
                st.error("ãƒ˜ãƒƒãƒ€è¡Œï¼ˆå¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ãªã©ï¼‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€ã¾ãŸã¯å¿…é ˆåˆ—ï¼ˆå¾—æ„å…ˆã‚³ãƒ¼ãƒ‰ã€å¾—æ„å…ˆåã€ç´”å£²ä¸Šé¡ï¼‰ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚Excelã®åˆ—æ§‹æˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
                st.stop()
            
            # --- ä¿®æ­£: è¨ˆç®—çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜ ---
            st.session_state.prev_clean = prev_clean
            st.session_state.curr_clean = curr_clean
            st.session_state.comp_df = compare_years(prev_clean, curr_clean)
            st.session_state.summary_df = summarize_by_category(st.session_state.comp_df)
            st.success("åˆ†æå®Œäº†ï¼")
            st.rerun() # è¨ˆç®—ãŒå®Œäº†ã—ãŸã‚‰ã€ãƒšãƒ¼ã‚¸ã‚’å†å®Ÿè¡Œã—ã¦çµæœã‚’è¡¨ç¤º

        except Exception as e:
            st.error(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
    else:
        st.info("ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ³ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")


# --- ä¿®æ­£: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã«ã®ã¿çµæœã‚’è¡¨ç¤º ---
if "comp_df" in st.session_state and "summary_df" in st.session_state:
    
    st.markdown("---") # è¦–è¦šçš„ãªåŒºåˆ‡ã‚Šç·š
    
    st.markdown("### Step 1: æ•´ç†å¾Œãƒ‡ãƒ¼ã‚¿")
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("å‰å¹´æ•´ç†ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(st.session_state.prev_clean, use_container_width=True)
    with col2:
        st.subheader("ä»Šå¹´æ•´ç†ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(st.session_state.curr_clean, use_container_width=True)

    st.markdown("---")
    
    st.markdown("### Step 2: å‰å¹´ vs ä»Šå¹´ æ¯”è¼ƒï¼ˆåƒå††å˜ä½ï¼‰")
    st.dataframe(st.session_state.comp_df, use_container_width=True)
    
    st.markdown("---")

    st.markdown("### Step 3: ä¸¦ã³æ›¿ãˆã¨é›†è¨ˆ")
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³åã‚’å…¨è§’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‹ã‚‰åŠè§’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ä¿®æ­£
    sort_options = (
        "å¤§åˆ†é¡åˆ¥_ç´”å£²ä¸Šé¡_ä»Šå¹´é †",
        "å¤§åˆ†é¡åˆ¥_å·®é¡ãƒ™ã‚¹ãƒˆé †",
        "å¤§åˆ†é¡åˆ¥_å·®é¡ãƒ¯ãƒ¼ã‚¹ãƒˆé †",
        "å¾—æ„å…ˆåˆ¥_ç´”å£²ä¸Šé¡_ä»Šå¹´é †",
        "å¾—æ„å…ˆåˆ¥_å·®é¡ãƒ™ã‚¹ãƒˆé †",
        "å¾—æ„å…ˆåˆ¥_å·®é¡ãƒ¯ãƒ¼ã‚¹ãƒˆé †",
    )
    option = st.selectbox(
        "ä¸¦ã³æ›¿ãˆåŸºæº–ã‚’é¸ã‚“ã§ãã ã•ã„",
        sort_options,
        key="sort_option_select"
    )

    # é¸æŠã•ã‚ŒãŸã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åŸºã¥ã„ã¦å‡¦ç†ã‚’åˆ†å²
    if option.startswith("å¤§åˆ†é¡åˆ¥"):
        summary_df = st.session_state.summary_df.copy() # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚’ä¿æŒã™ã‚‹ãŸã‚ã‚³ãƒ”ãƒ¼
        if "_ç´”å£²ä¸Šé¡_" in option:
            summary_sorted = summary_df.sort_values("ç´”å£²ä¸Šé¡_ä»Šå¹´", ascending=False)
        elif "ãƒ™ã‚¹ãƒˆ" in option:
            summary_sorted = summary_df.sort_values("å·®é¡", ascending=False)
        else: # ãƒ¯ãƒ¼ã‚¹ãƒˆ
            summary_sorted = summary_df.sort_values("å·®é¡", ascending=True)
        
        st.subheader("å¤§åˆ†é¡åˆ¥ï¼šé›†è¨ˆçµæœ")
        if not summary_sorted.empty:
            st.dataframe(summary_sorted, use_container_width=True)
            st.bar_chart(summary_sorted.set_index("å¤§åˆ†é¡")["ç´”å£²ä¸Šé¡_ä»Šå¹´"])
        else:
            st.info("é›†è¨ˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else: # å¾—æ„å…ˆåˆ¥
        comp_df = st.session_state.comp_df.copy() # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚’ä¿æŒã™ã‚‹ãŸã‚ã‚³ãƒ”ãƒ¼
        if "_ç´”å£²ä¸Šé¡_" in option:
            df_sorted = comp_df.sort_values("ç´”å£²ä¸Šé¡_ä»Šå¹´", ascending=False)
        elif "ãƒ™ã‚¹ãƒˆ" in option:
            df_sorted = comp_df.sort_values("å·®é¡", ascending=False)
        else: # ãƒ¯ãƒ¼ã‚¹ãƒˆ
            df_sorted = comp_df.sort_values("å·®é¡", ascending=True)
        
        st.subheader("å¾—æ„å…ˆåˆ¥ï¼šæ¯”è¼ƒçµæœ")
        if not df_sorted.empty:
            st.dataframe(df_sorted, use_container_width=True)
        else:
            st.info("é›†è¨ˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
